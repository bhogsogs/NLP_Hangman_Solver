{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6292351,"sourceType":"datasetVersion","datasetId":3618937},{"sourceId":7958392,"sourceType":"datasetVersion","datasetId":4681327},{"sourceId":8016004,"sourceType":"datasetVersion","datasetId":4722799},{"sourceId":8022161,"sourceType":"datasetVersion","datasetId":4727264}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"    ## imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MaskedLanguageModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3):\n        super(MaskedLanguageModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        # Embedding layer\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        \n        # Define multiple LSTM layers\n        self.lstms = nn.ModuleList([nn.LSTM(hidden_size, hidden_size, batch_first=True) for _ in range(num_layers)])\n        \n        # Output layer\n        self.fc = nn.Linear(hidden_size, input_size)\n        self.softmax = nn.Softmax(dim=1)\n        \n    def forward(self, input):\n        # Embed input\n        embedded = self.embedding(input)\n        \n        # Forward pass through LSTM layers\n        output = embedded\n        for lstm in self.lstms:\n            output, _ = lstm(output)\n        \n        # Output layer\n        output = self.fc(output[:, -1, :])  # Taking the last time-step's output\n        output = self.softmax(output)\n        return output\n\n# Example usage:\ninput_size = 100  # Example vocabulary size\nhidden_size = 128  # Example hidden size\nnum_layers = 3  # Number of LSTM layers\n\nmodel = MaskedLanguageModel(input_size, hidden_size, num_layers)\nprint(model)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, masked_words, original_words, char_to_index):\n        self.masked_words = masked_words\n        self.original_words = original_words\n        self.char_to_index = char_to_index\n\n    def __len__(self):\n        return len(self.masked_words)\n\n    def __getitem__(self, idx):\n        masked_word = self.masked_words[idx]\n        original_word = self.original_words[idx]\n        \n        # Convert characters to indices\n        masked_indices = [self.char_to_index[c] for c in masked_word]\n        \n        # Convert original word to soft encoding\n        weights = [0] * len(self.char_to_index)\n        total_diff = 0\n        for i in range(len(original_word)):\n            if original_word[i] != masked_word[i]:\n                weights[self.char_to_index[original_word[i]]] += 1 \n                total_diff += 1\n        \n        # Handle division by zero\n        if total_diff == 0:\n            weights = [1 / len(self.char_to_index)] * len(self.char_to_index)\n        else:\n            for i in range(len(weights)):\n                weights[i] = weights[i] / total_diff\n        \n        original_indices = torch.tensor(weights)\n        \n        return torch.tensor(masked_indices), original_indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Collate function for DataLoader\ndef collate_fn(batch):\n    masked_words, original_words = zip(*batch)\n    max_len = max(len(word) for word in masked_words)\n    padded_masked_words = torch.stack([torch.nn.functional.pad(word, (0, max_len - len(word)), value=0) for word in masked_words])\n    stacked_orignal_words = torch.stack(original_words)\n    # padded_original_words = torch.stack([torch.nn.functional.pad(word, (0, max_len - len(word)), value=0) for word in original_words])\n    return padded_masked_words, stacked_orignal_words","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\ndef generate_masked_words(original_words, mask_prob=0.6):\n    masked_words = []\n    for word in original_words:\n        masked_word = ''\n        first_char_unmasked = True  # Flag to keep track of the first character\n        for char in word:\n            if char.isalpha():\n                if first_char_unmasked:\n                    masked_word += char\n                    first_char_unmasked = False\n                elif random.random() <= mask_prob:\n                    masked_word += '_'\n                else:\n                    masked_word += char\n            else:\n                masked_word += char\n        masked_words.append(masked_word)\n    return masked_words\n  \n\n\n# Example usage:\nfile_path = '/kaggle/input/training-words/training.txt'  # Path to your text file containing words\n\n# Read the file and extract words\nwith open(file_path, 'r') as file:\n    words = [line.strip() for line in file]\n\n# Considering the first 4 words for demonstration\noriginal_words = words\n\n# Generate masked words\nmasked_words = generate_masked_words(original_words)\n\nprint(\"Original Words:\", original_words)\nprint(\"Masked Words:\", masked_words)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample data\n# masked_words = ['a__l_', 'p__', 'l__r___g', 'bott_e']\n# original_words = ['apple', 'pen', 'learning', 'bottle']\n\n## storing how words are mapped so that we can convert them back again\nchar_to_index = {chr(i): i - 96 for i in range(97, 123)}\nchar_to_index.update({'_': 27})\nchar_to_index.update({'-': 0})\nindex_to_char = {i: char for char, i in char_to_index.items()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training function\ndef train(model, dataloader, criterion, optimizer, epochs):\n    model.train()\n    for epoch in range(epochs):\n        running_loss = 0.0\n        for masked_words, weights_chars in tqdm(dataloader, desc=f'Epoch {epoch + 1}/{epochs}', unit='batch'):\n            optimizer.zero_grad()\n            output = model(masked_words)\n            loss = criterion(output, weights_chars)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(dataloader)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"GPU available, using GPU.\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, using CPU.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Hyperparameters\ninput_size = len(char_to_index)\nhidden_size = 10\nbatch_size = 32\nepochs = 2000\nlearning_rate = 0.001\nimport torch.nn.functional as F\n# Initialize model, criterion, optimizer\nmodel = MaskedLanguageModel(input_size, hidden_size)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Create DataLoader\ndataset = CustomDataset(masked_words, original_words, char_to_index)\ndataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)\n\n# Train the model\ntrain(model, dataloader, criterion, optimizer, epochs)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing function\ndef test(model, dataloader, char_to_index, index_to_char):\n    model.eval()\n    with torch.no_grad():\n        for masked_words, original_words in dataloader:\n            output = model(masked_words)\n            print(output)\n            char_ind = [np.argmax(output[i].detach().numpy()) for i in range(output.shape[0])]\n            predictions = [index_to_char[index] for index in char_ind]\n            print(predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample data\nmasked_words_test = ['c__k', 'b_t']\noriginal_words_test = ['cook', 'bat']\n\n# Create DataLoader\ndataset = CustomDataset(masked_words_test, original_words_test, char_to_index)\ndataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)\n# Test the model\ntest(model, dataloader, char_to_index, index_to_char)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}